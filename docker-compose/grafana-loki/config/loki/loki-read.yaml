auth_enabled: true

server:
  http_listen_address: 0.0.0.0
  grpc_listen_address: 0.0.0.0
  http_listen_port: 3100
  grpc_listen_port: 9095
  log_level: debug

# Common configuration to be shared between multiple modules.
# If a more specific configuration is given in other sections,
# the related configuration within this section will be ignored.
common:
  replication_factor: 1 # must be set number of minimum replication factor
  path_prefix: /loki # Update this accordingly, data will be stored here.
  ring:
    kvstore:
      store: memberlist

limits_config:
  unordered_writes: true

memberlist:
  advertise_port: 7946
  join_members:
    # You can use a headless k8s service for all distributor, ingester and querier components.
    - loki-write:7946 # :7946 is the default memberlist port.
    - loki-read:7946

schema_config:
  configs:
    - from: 2020-05-15
      store: tsdb
      object_store: s3
      schema: v12
      index:
        prefix: my-logs_
        period: 24h

storage_config:
  tsdb_shipper:
    active_index_directory: /loki/index
    cache_location: /loki/index_cache
    shared_store: s3
  aws:
    s3: http://minioadmin:minioadmin@minio.:9000/loki
    s3forcepathstyle: true
    insecure: true
    bucketnames: loki-data
    access_key_id: "minio-access-key"
    secret_access_key: "minio-secret-key"

## The BoltDB compactor service will run as part of the read target.
# https://github.com/grafana/loki/blob/v2.8.3/docs/sources/fundamentals/architecture/deployment-modes/_index.md?plain=1#L62
# Note: There should be only 1 compactor instance running at a time that
# otherwise could create problems and may lead to data loss.
compactor:
  working_directory: /loki/compactor
  shared_store: s3
  compaction_interval: 5m

## ===== CONFIG FOR READ REPLICA ONLY

query_range:
  # make queries more cache-able by aligning them with their step intervals
  align_queries_with_step: true
  max_retries: 5
  parallelise_shardable_queries: true
  cache_results: true

frontend:
  log_queries_longer_than: 5s
  compress_responses: true
  max_outstanding_per_tenant: 2048

query_scheduler:
  # the TSDB index dispatches many more, but each individually smaller, requests.
  # We increase the pending request queue sizes to compensate.
  max_outstanding_requests_per_tenant: 32768

querier:
  # Each `querier` component process runs a number of parallel workers to process queries simultaneously.
  # You may want to adjust this up or down depending on your resource usage
  # (more available cpu and memory can tolerate higher values and vice versa),
  # but we find the most success running at around `16` with tsdb
  max_concurrent: 16
  query_ingesters_within: 0
